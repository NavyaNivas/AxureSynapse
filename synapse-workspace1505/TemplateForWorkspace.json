{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapse-workspace1505"
		},
		"SecondaryStorageConnection_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SecondaryStorageConnection'"
		},
		"synapse-workspace1505-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapse-workspace1505-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapse-workspace1505.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapse-workspace1505-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsestorage1505.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/SecondaryStorageConnection')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Linked service to connect to secondary storage account ",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('SecondaryStorageConnection_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse-workspace1505-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapse-workspace1505-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse-workspace1505-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapse-workspace1505-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--create database\ncreate DATABASE demoDb\nGO\n\n--Use this database as primary..by default master will be there\nuse demoDb\nGo\n\n\n--Create master key encryption usinh scoped credential\ncreate MASTER KEY ENCRYPTION BY PASSWORD = 'Password@123'\nGO\n\nCREATE DATABASE SCOPED CREDENTIAL myCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nsecret = 'sas token'\n--this secret key can be found inside storage account in shared access signature-genaerate sas token--\nGO\n\n\n--create external datasource\ncreate EXTERNAL DATA SOURCE  myDataSource with(\n\n    LOCATION = 'https://synapsestorage1505.blob.core.windows.net/',   --location can be found in accountstorage level-settings-primaryendpoint--\n    CREDENTIAL = myCredential\n)\nGO\n\n\n--create extrenal file format\nCREATE EXTERNAL FILE FORMAT parquetFileFormat WITH(\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n\n--create schema\ncreate SCHEMA NYCTaxi\nGO\n\n\n--create external table inside the schema(cetas)\nCREATE EXTERNAL TABLE NYCTaxi.passengerDetails WITH(\n    LOCATION = 'synapsecontainer/NYCTaxi/DATA',               --location inside the synapse container--\n    DATA_SOURCE = myDataSource,\n    FILE_FORMAT = parquetFileFormat\n)\nAS\nSELECT TOP 100 * FROM\nOPENROWSET(\n\n    BULK 'https://synapsestorage1505.dfs.core.windows.net/synapsecontainer/Data/NYCTripSmall.parquet',\n    FORMAT = 'parquet'\n) \nAS[result]\nGO\n\n--to check the data in the extrenal db\nselect * from NYCTaxi.passengerDetails\n\n--can also do this by reading the parquet file in the extrnal datasource location..\nSELECT TOP 100 * FROM\nOPENROWSET(\n\n    BULK 'https://synapsestorage1505.dfs.core.windows.net/synapsecontainer/NYCTaxi/DATA/AC33B604-2030-4E4C-ACB8-3F4E5256968F_2_0-1.parquet',\n    FORMAT = 'parquet'\n) \nAS[result]\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demoDb",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--create database\ncreate DATABASE demoDb1\nGO\n\n--Use this database as primary..by default master will be there\nuse demoDb1\nGo\n\n\n--Create master key encryption using Managed identity--no need to pass the storage account sas token here..\ncreate MASTER KEY ENCRYPTION BY PASSWORD = 'Password@123'\nGO\n\nCREATE DATABASE SCOPED CREDENTIAL myCredential\nWITH IDENTITY = 'Managed identity'\nGO\n\n\n--create external datasource\ncreate EXTERNAL DATA SOURCE  myDataSource with(\n\n    LOCATION = 'https://synapsestorage1505.blob.core.windows.net/',   --location can be found in accountstorage level-settings-primaryendpoint--\n    CREDENTIAL = myCredential\n)\nGO\n\n\n--create extrenal file format\nCREATE EXTERNAL FILE FORMAT parquetFileFormat WITH(\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n\n--create schema\ncreate SCHEMA NYCTaxi\nGO\n\n\n--create external table inside the schema(cetas)\n\nCREATE EXTERNAL TABLE NYCTaxi.passengerDetails WITH(\n    LOCATION = 'synapsecontainer/NYCTaxi_2/DATA',               --location inside the synapse container--\n    DATA_SOURCE = myDataSource,\n    FILE_FORMAT = parquetFileFormat\n)\nAS\nSELECT TOP 100 * FROM\nOPENROWSET(\n\n    BULK 'https://synapsestorage1505.dfs.core.windows.net/synapsecontainer/Data/NYCTripSmall.parquet',\n    FORMAT = 'parquet'\n) \nAS[result]\nGO\n\n--to check the data in the extrenal db\nselect * from NYCTaxi.passengerDetails\n\n--can also do this by reading the parquet file in the extrnal datasource location..\nSELECT TOP 100 * FROM\nOPENROWSET(\n\n    BULK 'https://synapsestorage1505.dfs.core.windows.net/synapsecontainer/NYCTaxi_2/DATA/98A04E24-7493-49DF-A1CB-30561682C3CA_6_0-1.parquet',\n    FORMAT = 'parquet'\n) \nAS[result]\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demoDb1",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--First create dedicated sql pool and then connect to it while writing the script\n--create table inside the db..\nCREATE TABLE [dbo].[employees]\n(\n    empID INT NOT NULL,\n    empName NVARCHAR(50),\n    gender NVARCHAR(20),\n    dept NVARCHAR(20)\n)\nWITH\n(\n   DISTRIBUTION = HASH(empID),\n   CLUSTERED COLUMNSTORE index\n);\nGO\n\n\n--inserting data into the table\nINSERT INTO [dbo].[employees] VALUES (1,'Navya','Female','IT');\nINSERT INTO [dbo].[employees] VALUES (2,'Gaanavi','Female','HR');\nINSERT INTO [dbo].[employees] VALUES (3,'Sahasra','Female','Sales');\nINSERT INTO [dbo].[employees] VALUES (4,'Nikshit','Male','Sales');\nINSERT INTO [dbo].[employees] VALUES (5,'Chaitanya','Male','Research');\nINSERT INTO [dbo].[employees] VALUES (6,'Ramya','Female','HR');\nINSERT INTO [dbo].[employees] VALUES (7,'Phani','Male','IT');\nINSERT INTO [dbo].[employees] VALUES (8,'Gayatri','Female','IT');\nINSERT INTO [dbo].[employees] VALUES (9,'Srinidhi','Female','HR');\nINSERT INTO [dbo].[employees] VALUES (10,'Sri Maha','Female','Sales');\nGO\n\nselect * from [dbo].[employees] ORDER by empID\n\n\n--CTAS --creating another table by inserting the data from the previous table.. with same column for hashing\n\nCREATE TABLE [dbo].[employees_1]\nWITH\n(\n    DISTRIBUTION = HASH(empID),\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS \nSELECT * from [dbo].[employees] WHERE dept = 'IT' \nGO\n\n--checking the data inside the 2nd table\n\nselect * from [dbo].[employees_1] ORDER BY empID\n\n\n\n--CTAS --creating another table by inserting the data from the previous table.. with different column for hashing\n\nCREATE TABLE [dbo].[employees_2]\nWITH\n(\n    DISTRIBUTION = HASH(empName),\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS \nSELECT * from [dbo].[employees] WHERE dept = 'IT' \nGO\n\n--checking the data inside the 2nd table\n\nselect * from [dbo].[employees_2] ORDER BY empID\n\n\n\n--CTAS --creating another table by inserting the data from the previous table.. with different distribution\n\nCREATE TABLE [dbo].[employees_3]\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS \nSELECT * from [dbo].[employees] WHERE dept = 'IT' \nGO\n\n--checking the data inside the 2nd table\n\nselect * from [dbo].[employees_3] ORDER BY empID\n\n\n--can aslo create table using select into but we dont have option to change the disribution type and columnstore..Tbale will aslways be round robin distribution\n\nselect * INTO [dbo].[employees_4]\nFROM  [dbo].[employees] WHERE dept = 'IT' \nGO\n\nselect * from [dbo].[employees_4]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlserverPool",
						"poolName": "sqlserverPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--if we want to change the distribution type of the table we can do as below..\n\nCREATE TABLE FactInternetSales\n(\n    ProductKey int NOT NULL,\n    OrderDateKey int NOT NULL,\n    DueDateKey int NOT NULL,\n    ShipDateKey int NOT NULL,\n    CustomerKey int NOT NULL,\n    PromotionKey int NOT NULL,\n    CurrencyKey int NOT NULL,\n    SalesTerritoryKey int NOT NULL,\n    SalesOrderNumber nvarchar(20) NOT NULL,\n    SalesOrderLineNumber tinyint NOT NULL,\n    RevisionNumber tinyint NOT NULL,\n    OrderQuantity smallint NOT NULL,\n    UnitPrice money NOT NULL,\n    ExtendedAmount money NOT NULL,\n    UnitPriceDiscountPct float NOT NULL,\n    DiscountAmount float NOT NULL,\n    ProductStandardCost money NOT NULL,\n    TotalProductCost money NOT NULL,\n    SalesAmount money NOT NULL,\n    TaxAmt money NOT NULL,\n    Freight money NOT NULL,\n    CarrierTrackingNumber nvarchar(25),\n    CustomerPONumber nvarchar(25)\n)\nWITH( \n HEAP, \n DISTRIBUTION = ROUND_ROBIN \n);\n\n--creating new table with same schema s above table but this tym using distribution na d coulmnstore indexand partition\n--partion helps us to identify data with paration id so makes it easy to access..\n\nCREATE TABLE FactInternetSales_new\nWITH\n(\n    CLUSTERED COLUMNSTORE INDEX,\n    DISTRIBUTION = HASH(ProductKey),\n    PARTITION\n    (\n        OrderDateKey RANGE RIGHT FOR VALUES\n        (\n        20000101,20010101,20020101,20030101,20040101,20050101,20060101,20070101,20080101,20090101,\n        20100101,20110101,20120101,20130101,20140101,20150101,20160101,20170101,20180101,20190101,\n        20200101,20210101,20220101,20230101,20240101,20250101,20260101,20270101,20280101,20290101\n        )\n    )\n)\nAS SELECT * FROM FactInternetSales;\n\n--here renaming and dropping the table and finally we have table with distribution type as hash..can check in the icon against the table in the data tab..\n\nRENAME OBJECT [dbo].[FactInternetSales] TO FactInternetSales_old\nRENAME object FactInternetSales_new to FactInternetSales\ndrop  TABLE FactInternetSales_old\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlserverPool",
						"poolName": "sqlserverPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--creating external table in dediacted sql server pool\n\n--step 1: create external datasource..\n\nCREATE DATABASE SCOPED CREDENTIAL myCredential\nWITH IDENTITY = 'Managed identity'\nGO\n\n--step 2: create external datasource\n--if we want to cretae native external table then in the below script remove type..\n\nCREATE EXTERNAL data source myDataSource WITH\n(\n    LOCATION = 'abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net',\n    CREDENTIAL = myCredential,\n    TYPE = HADOOP\n)\nGO\n\n\n--stpe 3: create external file format..\n\nCREATE EXTERNAL FILE FORMAT parquetFileFormat WITH(\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n--step 5: create external tabl\n\n\n\nCREATE EXTERNAL TABLE dbo.ext_table_hdp (\n\t[DateID] int,\n\t[MedallionID] int,\n\t[HackneyLicenseID] int,\n\t[PickupTimeID] int,\n\t[DropoffTimeID] int,\n\t[PickupGeographyID] int,\n\t[DropoffGeographyID] int,\n\t[PickupLatitude] float,\n\t[PickupLongitude] float,\n\t[PickupLatLong] nvarchar(4000),\n\t[DropoffLatitude] float,\n\t[DropoffLongitude] float,\n\t[DropoffLatLong] nvarchar(4000),\n\t[PassengerCount] int,\n\t[TripDurationSeconds] int,\n\t[TripDistanceMiles] float,\n\t[PaymentType] nvarchar(4000),\n\t[FareAmount] numeric(19,4),\n\t[SurchargeAmount] numeric(19,4),\n\t[TaxAmount] numeric(19,4),\n\t[TipAmount] numeric(19,4),\n\t[TollsAmount] numeric(19,4),\n\t[TotalAmount] numeric(19,4)\n\t)\n\tWITH (\n\tLOCATION = '/Data/NYCTripSmall.parquet',\n\tDATA_SOURCE = myDataSource,\n\tFILE_FORMAT = parquetFileFormat\n\t)\nGO\n\nSELECT * FROM dbo.ext_table_hdp\n\n\n\n--stpes to create native external table..\n\nCREATE DATABASE SCOPED CREDENTIAL myCredential1\nWITH IDENTITY = 'Managed identity'\nGO\n\n--step 2: create external datasource\n--(if we want to cretae native external table then in the below script remove type..)\n\nCREATE EXTERNAL data source myDataSource1 WITH\n(\n    LOCATION = 'abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net',\n    CREDENTIAL = myCredential1\n   \n)\nGO\n\n\n--stpe 3: create external file format..\n\nCREATE EXTERNAL FILE FORMAT parquetFileFormat1 WITH(\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n--step 5: create external tabl\n\n\n\nCREATE EXTERNAL TABLE dbo.ext_table_native (\n\t[DateID] int,\n\t[MedallionID] int,\n\t[HackneyLicenseID] int,\n\t[PickupTimeID] int,\n\t[DropoffTimeID] int,\n\t[PickupGeographyID] int,\n\t[DropoffGeographyID] int,\n\t[PickupLatitude] float,\n\t[PickupLongitude] float,\n\t[PickupLatLong] nvarchar(4000),\n\t[DropoffLatitude] float,\n\t[DropoffLongitude] float,\n\t[DropoffLatLong] nvarchar(4000),\n\t[PassengerCount] int,\n\t[TripDurationSeconds] int,\n\t[TripDistanceMiles] float,\n\t[PaymentType] nvarchar(4000),\n\t[FareAmount] numeric(19,4),\n\t[SurchargeAmount] numeric(19,4),\n\t[TaxAmount] numeric(19,4),\n\t[TipAmount] numeric(19,4),\n\t[TollsAmount] numeric(19,4),\n\t[TotalAmount] numeric(19,4)\n\t)\n\tWITH (\n\tLOCATION = '/Data/NYCTripSmall.parquet',\n\tDATA_SOURCE = myDataSource1,\n\tFILE_FORMAT = parquetFileFormat1\n\t)\nGO\n\n\n--comparing performance of hadoop and native tables::\n\nSELECT top 10* FROM dbo.ext_table_native\n\nselect top 10* from dbo.ext_table_hdp\n\n--native is faster when compared to hadoop..time to execute is less in native..\n\n--create table usning external tables through CTAS\n\n\n\ncreate TABLE dbo.tb1\nwith\n(\n\n    distribution = hash(MedallionID),\n    clustered columnstore index\n)\nas \nselect * from dbo.ext_table_hdp\nGO\n\nselect * from dbo.tb1\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlserver",
						"poolName": "sqlserver"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--creating external table in serverless sql server pool\n\n--step 1: create DB and then create external datasource..\n\ncreate DATABASE mydb\n\nuse mydb\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD  = 'Password@123'\n\nCREATE DATABASE SCOPED CREDENTIAL myCredential\nWITH IDENTITY = 'Managed identity'\nGO\n\n--step 2: create external datasource\n--if we want to cretae native external table then in the below script remove type..\n\nCREATE EXTERNAL data source myDataSource WITH\n(\n    LOCATION = 'abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net',\n    CREDENTIAL = myCredential\n   -- TYPE = HADOOP\n)\nGO\n\n\n--stpe 3: create external file format..\n\nCREATE EXTERNAL FILE FORMAT parquetFileFormat WITH(\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n--step 5: create external tabl\n\ncreate EXTERNAL TABLE dbo.ext_table_native \nWITH (\n\tLOCATION = '/Data1/NYCTripSmall.parquet',\n\tDATA_SOURCE = myDataSource,\n\tFILE_FORMAT = parquetFileFormat\n\t)\nAS\nSELECT top 100* from OPENROWSET(\n    BULK 'https://synapsestorage1505.dfs.core.windows.net/synapsecontainer/Data/NYCTripSmall.parquet',\n    FORMAT = 'parquet'\n)as [result]\nGO\n\n\nCREATE EXTERNAL TABLE dbo.ext_table_hdp (\n\t[DateID] int,\n\t[MedallionID] int,\n\t[HackneyLicenseID] int,\n\t[PickupTimeID] int,\n\t[DropoffTimeID] int,\n\t[PickupGeographyID] int,\n\t[DropoffGeographyID] int,\n\t[PickupLatitude] float,\n\t[PickupLongitude] float,\n\t[PickupLatLong] nvarchar(4000),\n\t[DropoffLatitude] float,\n\t[DropoffLongitude] float,\n\t[DropoffLatLong] nvarchar(4000),\n\t[PassengerCount] int,\n\t[TripDurationSeconds] int,\n\t[TripDistanceMiles] float,\n\t[PaymentType] nvarchar(4000),\n\t[FareAmount] numeric(19,4),\n\t[SurchargeAmount] numeric(19,4),\n\t[TaxAmount] numeric(19,4),\n\t[TipAmount] numeric(19,4),\n\t[TollsAmount] numeric(19,4),\n\t[TotalAmount] numeric(19,4)\n\t)\n\tWITH (\n\tLOCATION = '/Data/NYCTripSmall.parquet',\n\tDATA_SOURCE = myDataSource,\n\tFILE_FORMAT = parquetFileFormat\n\t)\nGO\n\nSELECT * FROM dbo.ext_table_hdp\n\n\n\n--stpes to create native external table..\n\nCREATE DATABASE SCOPED CREDENTIAL myCredential1\nWITH IDENTITY = 'Managed identity'\nGO\n\n--step 2: create external datasource\n--(if we want to cretae native external table then in the below script remove type..)\n\nCREATE EXTERNAL data source myDataSource1 WITH\n(\n    LOCATION = 'abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net',\n    CREDENTIAL = myCredential1\n   \n)\nGO\n\n\n--stpe 3: create external file format..\n\nCREATE EXTERNAL FILE FORMAT parquetFileFormat1 WITH(\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n--step 5: create external table using schema as well \n\n\n\nCREATE EXTERNAL TABLE dbo.ext_table_native (\n\t[DateID] int,\n\t[MedallionID] int,\n\t[HackneyLicenseID] int,\n\t[PickupTimeID] int,\n\t[DropoffTimeID] int,\n\t[PickupGeographyID] int,\n\t[DropoffGeographyID] int,\n\t[PickupLatitude] float,\n\t[PickupLongitude] float,\n\t[PickupLatLong] nvarchar(4000),\n\t[DropoffLatitude] float,\n\t[DropoffLongitude] float,\n\t[DropoffLatLong] nvarchar(4000),\n\t[PassengerCount] int,\n\t[TripDurationSeconds] int,\n\t[TripDistanceMiles] float,\n\t[PaymentType] nvarchar(4000),\n\t[FareAmount] numeric(19,4),\n\t[SurchargeAmount] numeric(19,4),\n\t[TaxAmount] numeric(19,4),\n\t[TipAmount] numeric(19,4),\n\t[TollsAmount] numeric(19,4),\n\t[TotalAmount] numeric(19,4)\n\t)\n\tWITH (\n\tLOCATION = '/Data/NYCTripSmall.parquet',\n\tDATA_SOURCE = myDataSource1,\n\tFILE_FORMAT = parquetFileFormat1\n\t)\nGO\n\n\n--we cannot create hadoop type external table in serverless pool..",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "mydb",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL_sereverless_pool_learning')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--select query to preview the data in external files..\n--for csv files parserversion and header row is mandatory to read the schema..\nselect * from OPENROWSET(\n    BULK  'abfss://input@synapsestorage1505.dfs.core.windows.net/employee_data.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    HEADER_ROW = TRUE\n)as [result]\n\nselect * from OPENROWSET(\n    BULK  'abfss://input@synapsestorage1505.dfs.core.windows.net/salary_data.parquet',\n    FORMAT = 'parquet'\n\n)as [result]\n\n--we can craete external tables only in serverless pool as there is no storage location involved with it..\n--create external table in  serverless pool..\n\nCREATE DATABASE myExternalDb\n\nUSE  myExternalDb\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Password@123'\nGO\n\n--using SAS token--\n\nCREATE DATABASE SCOPED CREDENTIAL myCredential \nWITH\nidentity = 'shared access signature',\nSECRET = 'sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-11-17T15:56:53Z&st=2025-11-17T07:41:53Z&spr=https&sig=Q5IcJLoKjwx%2FtRcQaOHvST54LXc0EFdQ3MbMFAzGVkk%3D'\nGO\n\n--using manged identity\n\nCREATE DATABASE SCOPED CREDENTIAL MyManagedIdentity\nWITH IDENTITY = 'Managed Identity'\nGO\n\n--can create data source with any one one of the credential..\n--location should be primary storage end point at the storage account level\n\nCREATE EXTERNAL DATA SOURCE externalDataSource \nWITH(\n    LOCATION = 'https://synapsestorage1505.blob.core.windows.net/',\n    CREDENTIAL = myCredential\n)\nGO\n\nCREATE EXTERNAL FILE FORMAT myExternalFileFormat\nWITH\n(\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n\n)GO\n\nCREATE EXTERNAL TABLE extTab1 WITH(\n\n    LOCATION = '/synapsecontainer/Externaldata', --no need to mention fiull path as it is altreday linked to primary storage account..\n    DATA_SOURCE = externalDataSource,\n    FILE_FORMAT = myExternalFileFormat\n\n)AS\nSELECT * from OPENROWSET(\n    BULK  'abfss://input@synapsestorage1505.dfs.core.windows.net/salary_data.parquet',\n    FORMAT = 'parquet'\n)AS [result]\nGO\n\nselect * from extTab1\n\n--similarly we can aslo create datasource and external file systema nd external table using managed identity..\n\nCREATE EXTERNAL DATA SOURCE externalDataSource1 \nWITH(\n    LOCATION = 'https://synapsestorage1505.blob.core.windows.net/',\n    CREDENTIAL = MyManagedIdentity\n)\nGO\n\n--compression type is not needed..\n\nCREATE EXTERNAL FILE FORMAT myExternalFileFormatCSV\nWITH\n(\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (FIELD_TERMINATOR = ',', STRING_DELIMITER = '\"')\n  \n)\nGO\n\n\nCREATE EXTERNAL TABLE extTab2 WITH(\n\n    LOCATION = '/synapsecontainer/Externaltable2', --no need to mention fiull path as it is altreday linked to primary storage account..\n    DATA_SOURCE = externalDataSource1,\n    FILE_FORMAT = myExternalFileFormatCSV\n\n)AS\nSELECT * from OPENROWSET(\n    BULK  'abfss://input@synapsestorage1505.dfs.core.windows.net/employee_data.csv',\n    FORMAT = 'csv',\n    PARSER_VERSION = '2.0',\n    HEADER_ROW = TRUE\n)AS [result]\nGO\n\nselect * from extTab2\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "myExternalDb",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1709dd4d-4352-4dbe-83e8-e3fe1b762831"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5273a484-920e-4cf4-b6e5-b107b5d28d31/resourceGroups/synapse-rg/providers/Microsoft.Synapse/workspaces/synapse-workspace1505/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-workspace1505.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**Reading a file from blob storage and write file into container**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**reading the file inside the primary storage account linked to the synapse workspace..**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.load(\"abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net/data/input/employee_data.csv\",format=\"csv\",header =True)\n",
							"df.show(10)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**Filter the data based on condition.**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_new=df.filter(df.deptID == 10)\n",
							"df_new.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**writing the new data into csv file**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_new.write.mode(\"overwrite\").csv(\"abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net/data/output/abc.csv\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**similarly we can read and write parquet files:**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.load(path=\"abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net/data/input/salary_data.parquet\",format=\"parquet\")\n",
							"df.show(10)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**Using filter function**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_new = df.filter(df.department==\"IT\")\n",
							"df_new.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**writing the filter data into parquet file in container 2**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_new.write.mode(\"overwrite\").parquet(path=\"abfss://synapsecontainer@synapsestorage1505.dfs.core.windows.net/data/output/dept.parquet\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cc67d3f0-b606-4c15-a129-f0b14f0975ae"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5273a484-920e-4cf4-b6e5-b107b5d28d31/resourceGroups/synapse-rg/providers/Microsoft.Synapse/workspaces/synapse-workspace1505/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-workspace1505.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**Reading the file from secondary storage account- we should add blob storage data contributor role in the secondary storage\n",
							"**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Hello\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"df=spark.read.load(path=\"abfss://input@secondarystorageaccount.dfs.core.windows.net/employee_data.csv\",format=\"csv\",header=True)\n",
							"df.show(10)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**write the data into file**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df.write.mode(\"overwrite\").csv(path =\"abfss://output@secondarystorageaccount.dfs.core.windows.net/employee_data.csv\")\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**Read and write opertains fro parquet file:**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df=spark.read.load(path=\"abfss://input@secondarystorageaccount.dfs.core.windows.net/salary_data.parquet\",format=\"parquet\",header=True)\n",
							"df.show(10)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"df_new = df.filter(df.department == \"IT\")\n",
							"df_new.show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"df.write.mode(\"overwrite\").parquet(path=\"abfss://output@secondarystorageaccount.dfs.core.windows.net/parquet\")"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7fa55dab-5f1c-4f56-98d5-10a2dba55233"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5273a484-920e-4cf4-b6e5-b107b5d28d31/resourceGroups/synapse-rg/providers/Microsoft.Synapse/workspaces/synapse-workspace1505/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-workspace1505.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
							""
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**Read and write opertaions in the default storage account using pandas library**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Hello\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"source": [
							"df=pd.read_csv('abfss://input@secondarystorageaccount.dfs.core.windows.net/employee_data.csv')\n",
							"print(df)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"df_new=df[df['deptID']==10]\n",
							"print(df_new)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"df_new.to_csv('abfss://output@secondarystorageaccount.dfs.core.windows.net/data.csv')"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**Read & write operations of parquet file usinh pandas**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"df = spark.read.load(path='abfss://input@secondarystorageaccount.dfs.core.windows.net/salary_data.parquet',format='parquet')\n",
							"df_new=df.toPandas()\n",
							"print(df_new)\n",
							""
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**write parquet file into secondary storage**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_new.to_parquet('abfss://output@secondarystorageaccount.dfs.core.windows.net/output/data.parquet')\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b907ba7e-8c71-4e5d-9629-936f329ccf99"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5273a484-920e-4cf4-b6e5-b107b5d28d31/resourceGroups/synapse-rg/providers/Microsoft.Synapse/workspaces/synapse-workspace1505/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-workspace1505.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**using code snippets**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Hello\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"        \n",
							"# Azure storage access info \n",
							"blob_account_name = 'secondarystorageaccount' # replace with your blob name \n",
							"blob_container_name = 'input' # replace with your container name \n",
							"blob_relative_path = '' # replace with your relative folder path \n",
							"linked_service_name = 'SecondaryStorageConnection' # replace with your linked service name \n",
							"        \n",
							"blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \n",
							"        \n",
							"# Allow SPARK to access from Blob remotely \n",
							"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \n",
							"print('Remote blob path: ' + wasbs_path) \n",
							"        \n",
							"# Read a csv file \n",
							"csv_path = wasbs_path + 'employee_data.csv' \n",
							"df_csv = spark.read.csv(csv_path, header = 'true') \n",
							"\n",
							"        \n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"df_csv.show()"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 5')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8a16dd61-04d8-4151-a0c8-6c90a523cacd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/5273a484-920e-4cf4-b6e5-b107b5d28d31/resourceGroups/synapse-rg/providers/Microsoft.Synapse/workspaces/synapse-workspace1505/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-workspace1505.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<mark>**File sytsem related code**</mark>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Hello\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.help()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.cp('/data/input/employee_data.csv','/data/output/employee_data.csv')"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.cp('/data/input','/data/output',True)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.exists('/data/input/employee_data.csv')"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.ls('/data/input')"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.fastcp('/data/input/employee_data.csv','/data/output/employee_data1.csv')"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.mv('/data/input/employee_data.csv','/data/output/employee_data2.csv')"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.mv('/data/input/','/data/output/input',True)"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.put('/data/input/test1.csv','1,sara',True)"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.append('/data/input/test1.csv','/n2,Jhon',True)"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.head('/data/input/test1.csv')\n",
							""
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.rm('/data/input/test1.csv')\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.ls('/data/input/')"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"source": [
							"files = mssparkutils.fs.ls('/data/input/')\n",
							"for f in files:{\n",
							"print(f.path), \n",
							"print(f.name),\n",
							"print(f.size)\n",
							"}\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"if ((mssparkutils.fs.exists('/data/input1'))==True):{\n",
							"print(\"Directory exist\")\n",
							"}\n",
							"print(\"Directory not exist\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.5",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}